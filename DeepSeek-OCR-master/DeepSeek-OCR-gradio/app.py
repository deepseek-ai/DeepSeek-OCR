import gradio as gr
import torch
from transformers import AutoModel, AutoTokenizer
import os
import tempfile
from PIL import Image, ImageDraw
import re
from typing import Tuple, Optional, Dict, Any

# --- å¸¸é‡å’Œé…ç½® ---
MODEL_NAME = "deepseek-ai/DeepSeek-OCR"
MODEL_SIZE_CONFIGS = {
    "Tiny": {"base_size": 512, "image_size": 512, "crop_mode": False},
    "Small": {"base_size": 640, "image_size": 640, "crop_mode": False},
    "Base": {"base_size": 1024, "image_size": 1024, "crop_mode": False},
    "Large": {"base_size": 1280, "image_size": 1280, "crop_mode": False},
    "Gundam (æ¨è)": {"base_size": 1024, "image_size": 640, "crop_mode": True},
}

TASK_PROMPTS = {
    "ğŸ“ è‡ªç”±OCR": "<image>\nè‡ªç”±OCR.",
    "ğŸ“„ è½¬æ¢ä¸ºMarkdown": "<image>\n<|grounding|>å°†æ–‡æ¡£è½¬æ¢ä¸ºmarkdown.",
    "ğŸ“ˆ è§£æå›¾è¡¨": "<image>\nè§£æå›¾è¡¨.",
}

DEFAULT_MODEL_SIZE = "Gundam (æ¨è)"
DEFAULT_TASK_TYPE = "ğŸ“„ è½¬æ¢ä¸ºMarkdown"
BOUNDING_BOX_PATTERN = re.compile(r"<\|det\|>\[\[(\d+),\s*(\d+),\s*(\d+),\s*(\d+)\]\]<\|/det\|>")
BOUNDING_BOX_COLOR = "red"
BOUNDING_BOX_WIDTH = 3
NORMALIZATION_FACTOR = 1000

# --- å…¨å±€å˜é‡ ---
model = None
tokenizer = None
model_gpu = None


def load_model_and_tokenizer() -> None:
    """å¯åŠ¨æ—¶åŠ è½½DeepSeek-OCRæ¨¡å‹å’Œåˆ†è¯å™¨ã€‚"""
    global model, tokenizer
    print("æ­£åœ¨åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨...")
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)
    model = AutoModel.from_pretrained(
        MODEL_NAME,
        _attn_implementation="flash_attention_2",
        trust_remote_code=True,
        use_safetensors=True,
    )
    model = model.eval()
    print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸã€‚")


def move_model_to_gpu() -> None:
    """å¦‚æœæ¨¡å‹å°šæœªåœ¨GPUä¸Šï¼Œåˆ™å°†å…¶ç§»åŠ¨åˆ°GPUã€‚"""
    global model_gpu
    if model_gpu is None:
        print("ğŸš€ æ­£åœ¨å°†æ¨¡å‹ç§»åŠ¨åˆ°GPU...")
        # ä½¿ç”¨éé˜»å¡ä¼ è¾“ä»¥è·å¾—æ›´å¥½çš„æ€§èƒ½
        model_gpu = model.cuda().to(torch.bfloat16, non_blocking=True)
        print("âœ… æ¨¡å‹å·²åœ¨GPUä¸Šã€‚")


def find_result_image(path: str) -> Optional[Image.Image]:
    """
    åœ¨æŒ‡å®šè·¯å¾„ä¸­æŸ¥æ‰¾é¢„ç”Ÿæˆçš„ç»“æœå›¾åƒã€‚
    
    Args:
        path: æœç´¢ç»“æœå›¾åƒçš„ç›®å½•è·¯å¾„
        
    Returns:
        å¦‚æœæ‰¾åˆ°åˆ™è¿”å›PILå›¾åƒï¼Œå¦åˆ™è¿”å›None
    """
    for filename in os.listdir(path):
        if "grounding" in filename or "result" in filename:
            try:
                image_path = os.path.join(path, filename)
                return Image.open(image_path)
            except Exception as e:
                print(f"æ‰“å¼€ç»“æœå›¾åƒ {filename} æ—¶å‡ºé”™: {e}")
    return None


def build_prompt(task_type: str, ref_text: str) -> str:
    """
    æ ¹æ®ä»»åŠ¡ç±»å‹å’Œå‚è€ƒæ–‡æœ¬æ„å»ºé€‚å½“çš„æç¤ºã€‚
    
    Args:
        task_type: OCRä»»åŠ¡ç±»å‹
        ref_text: å®šä½ä»»åŠ¡çš„å‚è€ƒæ–‡æœ¬
        
    Returns:
        æ ¼å¼åŒ–çš„æç¤ºå­—ç¬¦ä¸²
    """
    if task_type == "ğŸ” é€šè¿‡å‚è€ƒå®šä½å¯¹è±¡":
        if not ref_text or ref_text.strip() == "":
            raise gr.Error("å¯¹äº'å®šä½'ä»»åŠ¡ï¼Œæ‚¨å¿…é¡»æä¾›è¦æŸ¥æ‰¾çš„å‚è€ƒæ–‡æœ¬ï¼")
        return f"<image>\nåœ¨å›¾åƒä¸­å®šä½ <|ref|>{ref_text.strip()}<|/ref|>."
    
    return TASK_PROMPTS.get(task_type, TASK_PROMPTS["ğŸ“ è‡ªç”±OCR"])


def extract_and_draw_bounding_boxes(text_result: str, original_image: Image.Image) -> Optional[Image.Image]:
    """
    ä»æ–‡æœ¬ç»“æœä¸­æå–è¾¹ç•Œæ¡†åæ ‡å¹¶åœ¨å›¾åƒä¸Šç»˜åˆ¶å®ƒä»¬ã€‚
    
    Args:
        text_result: åŒ…å«è¾¹ç•Œæ¡†åæ ‡çš„OCRæ–‡æœ¬ç»“æœ
        original_image: è¦ç»˜åˆ¶çš„åŸå§‹PILå›¾åƒ
        
    Returns:
        ç»˜åˆ¶äº†è¾¹ç•Œæ¡†çš„PILå›¾åƒï¼Œå¦‚æœæ²¡æœ‰æ‰¾åˆ°åæ ‡åˆ™è¿”å›None
    """
    # ç›´æ¥ä½¿ç”¨è¿­ä»£å™¨ä»¥é¿å…ä¸å¿…è¦åœ°åˆ›å»ºåˆ—è¡¨
    matches = list(BOUNDING_BOX_PATTERN.finditer(text_result))
    
    if not matches:
        return None
    
    print(f"âœ… æ‰¾åˆ° {len(matches)} ä¸ªè¾¹ç•Œæ¡†ã€‚æ­£åœ¨åŸå§‹å›¾åƒä¸Šç»˜åˆ¶ã€‚")
    
    # åˆ›å»ºåŸå§‹å›¾åƒçš„å‰¯æœ¬ä»¥è¿›è¡Œç»˜åˆ¶
    image_with_bboxes = original_image.copy()
    draw = ImageDraw.Draw(image_with_bboxes)
    w, h = original_image.size
    
    # é¢„å…ˆè®¡ç®—ç¼©æ”¾å› å­ä»¥è·å¾—æ›´å¥½çš„æ€§èƒ½
    w_scale = w / NORMALIZATION_FACTOR
    h_scale = h / NORMALIZATION_FACTOR
    
    for match in matches:
        # æ›´æœ‰æ•ˆåœ°æå–å’Œç¼©æ”¾åæ ‡
        coords = tuple(int(c) for c in match.groups())
        x1_norm, y1_norm, x2_norm, y2_norm = coords
        
        # ä½¿ç”¨é¢„å…ˆè®¡ç®—çš„å› å­ç¼©æ”¾å½’ä¸€åŒ–åæ ‡
        x1 = int(x1_norm * w_scale)
        y1 = int(y1_norm * h_scale)
        x2 = int(x2_norm * w_scale)
        y2 = int(y2_norm * h_scale)
        
        # ç»˜åˆ¶çŸ©å½¢
        draw.rectangle([x1, y1, x2, y2], outline=BOUNDING_BOX_COLOR, width=BOUNDING_BOX_WIDTH)
    
    return image_with_bboxes


def run_inference(prompt: str, image_path: str, output_path: str, config: Dict[str, Any]) -> str:
    """
    ä½¿ç”¨ç»™å®šå‚æ•°è¿è¡Œæ¨¡å‹æ¨ç†ã€‚
    
    Args:
        prompt: æ¨¡å‹çš„æ ¼å¼åŒ–æç¤º
        image_path: è¾“å…¥å›¾åƒçš„è·¯å¾„
        output_path: è¾“å‡ºæ–‡ä»¶çš„ç›®å½•è·¯å¾„
        config: æ¨¡å‹é…ç½®å­—å…¸
        
    Returns:
        æ¨¡å‹çš„æ–‡æœ¬ç»“æœ
    """
    print(f"ğŸƒ ä½¿ç”¨æç¤ºè¿è¡Œæ¨ç†: {prompt}")
    text_result = model_gpu.infer(
        tokenizer,
        prompt=prompt,
        image_file=image_path,
        output_path=output_path,
        base_size=config["base_size"],
        image_size=config["image_size"],
        crop_mode=config["crop_mode"],
        save_results=True,
        test_compress=True,
        eval_mode=True,
    )
    print(f"====\nğŸ“„ æ–‡æœ¬ç»“æœ: {text_result}\n====")
    return text_result


def process_ocr_task(image: Optional[Image.Image], model_size: str, task_type: str, ref_text: str) -> Tuple[str, Optional[Image.Image]]:
    """
    ä½¿ç”¨DeepSeek-OCRå¤„ç†å›¾åƒä»¥æ”¯æŒæ‰€æœ‰ä»»åŠ¡ã€‚
    
    Args:
        image: è¾“å…¥PILå›¾åƒ
        model_size: æ¨¡å‹å¤§å°é…ç½®
        task_type: OCRä»»åŠ¡ç±»å‹
        ref_text: å®šä½ä»»åŠ¡çš„å‚è€ƒæ–‡æœ¬
        
    Returns:
        (text_result, result_image) å…ƒç»„
    """
    if image is None:
        return "è¯·å…ˆä¸Šä¼ å›¾åƒã€‚", None
    
    # ç¡®ä¿æ¨¡å‹åœ¨GPUä¸Š
    move_model_to_gpu()
    
    # æ ¹æ®ä»»åŠ¡ç±»å‹æ„å»ºæç¤º
    prompt = build_prompt(task_type, ref_text)
    
    # è·å–æ¨¡å‹é…ç½®
    config = MODEL_SIZE_CONFIGS.get(model_size, MODEL_SIZE_CONFIGS[DEFAULT_MODEL_SIZE])
    
    with tempfile.TemporaryDirectory() as output_path:
        # ä½¿ç”¨ä¼˜åŒ–æ ¼å¼ä¿å­˜ä¸´æ—¶å›¾åƒ
        temp_image_path = os.path.join(output_path, "temp_image.png")
        # ä½¿ç”¨optimize=Trueä»¥è·å¾—æ›´å¥½çš„å‹ç¼©
        image.save(temp_image_path, optimize=True)
        
        # è¿è¡Œæ¨ç†
        text_result = run_inference(prompt, temp_image_path, output_path, config)
        
        # å°è¯•ä»æ–‡æœ¬ç»“æœä¸­æå–å¹¶ç»˜åˆ¶è¾¹ç•Œæ¡†
        result_image = extract_and_draw_bounding_boxes(text_result, image)
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°è¾¹ç•Œæ¡†ï¼Œåˆ™å›é€€åˆ°é¢„ç”Ÿæˆçš„ç»“æœå›¾åƒ
        if result_image is None:
            print("âš ï¸ åœ¨æ–‡æœ¬ç»“æœä¸­æœªæ‰¾åˆ°è¾¹ç•Œæ¡†åæ ‡ã€‚å›é€€åˆ°æœç´¢ç»“æœå›¾åƒæ–‡ä»¶ã€‚")
            result_image = find_result_image(output_path)
        
        return text_result, result_image


def toggle_ref_text_visibility(task: str) -> gr.Textbox:
    """
    æ ¹æ®ä»»åŠ¡ç±»å‹åˆ‡æ¢å‚è€ƒæ–‡æœ¬è¾“å…¥çš„å¯è§æ€§ã€‚
    
    Args:
        task: é€‰å®šçš„ä»»åŠ¡ç±»å‹
        
    Returns:
        æ›´æ–°çš„Textboxç»„ä»¶
    """
    return gr.Textbox(visible=True) if task == "ğŸ” é€šè¿‡å‚è€ƒå®šä½å¯¹è±¡" else gr.Textbox(visible=False)


def create_ui() -> gr.Blocks:
    """
    åˆ›å»ºå’Œé…ç½®Gradioç”¨æˆ·ç•Œé¢ã€‚
    
    Returns:
        é…ç½®å¥½çš„Gradio Blocksç•Œé¢
    """
    with gr.Blocks(title="ğŸ³DeepSeek-OCRğŸ³", theme=gr.themes.Soft()) as demo:
        gr.Markdown(
            """
            # ğŸ³ DeepSeek-OCR å®Œæ•´æ¼”ç¤º ğŸ³
            **ğŸ’¡ ä½¿ç”¨æ–¹æ³•:**
            1.  ä½¿ç”¨ä¸Šä¼ æ¡†**ä¸Šä¼ å›¾åƒ**ã€‚
            2.  é€‰æ‹©ä¸€ä¸ª**åˆ†è¾¨ç‡**ã€‚å¯¹äºå¤§å¤šæ•°æ–‡æ¡£ï¼Œæ¨èä½¿ç”¨`Gundam`ã€‚
            3.  é€‰æ‹©ä¸€ä¸ª**ä»»åŠ¡ç±»å‹**:
                - **ğŸ“ è‡ªç”±OCR**: ä»å›¾åƒä¸­æå–åŸå§‹æ–‡æœ¬ã€‚
                - **ğŸ“„ è½¬æ¢ä¸ºMarkdown**: å°†æ–‡æ¡£è½¬æ¢ä¸ºMarkdownï¼Œä¿ç•™ç»“æ„ã€‚
                - **ğŸ“ˆ è§£æå›¾è¡¨**: ä»å›¾è¡¨å’Œå›¾å½¢ä¸­æå–ç»“æ„åŒ–æ•°æ®ã€‚
                - **ğŸ” é€šè¿‡å‚è€ƒå®šä½å¯¹è±¡**: æŸ¥æ‰¾ç‰¹å®šå¯¹è±¡/æ–‡æœ¬ã€‚
            4. å¦‚æœè¿™ä¸ªå·¥å…·æœ‰å¸®åŠ©ï¼Œè¯·ç»™å®ƒç‚¹ä¸ªèµï¼ ğŸ™ â¤ï¸
            """
        )

        with gr.Row():
            with gr.Column(scale=1):
                image_input = gr.Image(type="pil", label="ğŸ–¼ï¸ ä¸Šä¼ å›¾åƒ", sources=["upload", "clipboard"])
                model_size = gr.Dropdown(
                    choices=list(MODEL_SIZE_CONFIGS.keys()),
                    value=DEFAULT_MODEL_SIZE,
                    label="âš™ï¸ åˆ†è¾¨ç‡å¤§å°"
                )
                task_type = gr.Dropdown(
                    choices=list(TASK_PROMPTS.keys()) + ["ğŸ” é€šè¿‡å‚è€ƒå®šä½å¯¹è±¡"],
                    value=DEFAULT_TASK_TYPE,
                    label="ğŸš€ ä»»åŠ¡ç±»å‹"
                )
                ref_text_input = gr.Textbox(
                    label="ğŸ“ å‚è€ƒæ–‡æœ¬ï¼ˆç”¨äºå®šä½ä»»åŠ¡ï¼‰",
                    placeholder="ä¾‹å¦‚ï¼šè€å¸ˆã€20-10ã€ä¸€è¾†çº¢è‰²æ±½è½¦...",
                    visible=False
                )
                submit_btn = gr.Button("å¤„ç†å›¾åƒ", variant="primary")

            with gr.Column(scale=2):
                output_text = gr.Textbox(label="ğŸ“„ æ–‡æœ¬ç»“æœ", lines=15, show_copy_button=True)
                output_image = gr.Image(label="ğŸ–¼ï¸ å›¾åƒç»“æœï¼ˆå¦‚æœæœ‰ï¼‰", type="pil")

        # UIäº¤äº’é€»è¾‘
        task_type.change(fn=toggle_ref_text_visibility, inputs=task_type, outputs=ref_text_input)
        submit_btn.click(
            fn=process_ocr_task,
            inputs=[image_input, model_size, task_type, ref_text_input],
            outputs=[output_text, output_image]
        )

        # ç¤ºä¾‹å›¾åƒå’Œä»»åŠ¡
        gr.Examples(
            examples=[
                ["doc_markdown.png", "will upload", "ğŸ“„ will upload", ""],
            ],
            inputs=[image_input, model_size, task_type, ref_text_input],
            outputs=[output_text, output_image],
            fn=process_ocr_task,
            cache_examples=False,  # ç¦ç”¨ç¼“å­˜ä»¥ç¡®ä¿ç¤ºä¾‹æ¯æ¬¡éƒ½è¿è¡Œ
        )
    
    return demo


def main() -> None:
    """åˆå§‹åŒ–å’Œå¯åŠ¨åº”ç”¨ç¨‹åºçš„ä¸»å‡½æ•°ã€‚"""
    # å¯åŠ¨æ—¶åŠ è½½æ¨¡å‹
    load_model_and_tokenizer()
    
    # å¦‚æœç¤ºä¾‹ç›®å½•ä¸å­˜åœ¨åˆ™åˆ›å»º
    if not os.path.exists("examples"):
        os.makedirs("examples")
    
    # åˆ›å»ºå¹¶å¯åŠ¨UI
    demo = create_ui()
    demo.queue(max_size=20).launch(share=True)


if __name__ == "__main__":
    main()