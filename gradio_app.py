import asyncio
import re
import os
import sys
from pathlib import Path
import threading

import torch
if torch.version.cuda == '11.8':
    os.environ["TRITON_PTXAS_PATH"] = "/usr/local/cuda-11.8/bin/ptxas"

os.environ['VLLM_USE_V1'] = '0'

import gradio as gr
from vllm import AsyncLLMEngine, SamplingParams
from vllm.engine.arg_utils import AsyncEngineArgs
from vllm.model_executor.models.registry import ModelRegistry
import time
from PIL import Image, ImageDraw, ImageFont, ImageOps
import numpy as np

# æ·»åŠ DeepSeek-OCR-vllmåˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent / "DeepSeek-OCR-master" / "DeepSeek-OCR-vllm"))

from deepseek_ocr import DeepseekOCRForCausalLM
from process.ngram_norepeat import NoRepeatNGramLogitsProcessor
from process.image_process import DeepseekOCRProcessor
from config import MODEL_PATH

ModelRegistry.register_model("DeepseekOCRForCausalLM", DeepseekOCRForCausalLM)

# å…¨å±€å˜é‡å­˜å‚¨å¼•æ“å’Œäº‹ä»¶å¾ªç¯
engine = None
processor = None
event_loop = None
loop_thread = None
engine_lock = threading.Lock()


def load_image(image_input):
    """åŠ è½½å¹¶å¤„ç†å›¾åƒ"""
    try:
        if isinstance(image_input, str):
            image = Image.open(image_input)
        else:
            image = image_input

        # è‡ªåŠ¨æ—‹è½¬å›¾åƒ
        corrected_image = ImageOps.exif_transpose(image)
        return corrected_image.convert('RGB')
    except Exception as e:
        print(f"åŠ è½½å›¾åƒå‡ºé”™: {e}")
        return None


def re_match(text):
    """åŒ¹é…groundingæ ‡è®°"""
    pattern = r'(<\|ref\|>(.*?)<\|/ref\|><\|det\|>(.*?)<\|/det\|>)'
    matches = re.findall(pattern, text, re.DOTALL)

    matches_image = []
    matches_other = []
    for a_match in matches:
        if '<|ref|>image<|/ref|>' in a_match[0]:
            matches_image.append(a_match[0])
        else:
            matches_other.append(a_match[0])
    return matches, matches_image, matches_other


def extract_coordinates_and_label(ref_text, image_width, image_height):
    """æå–åæ ‡å’Œæ ‡ç­¾"""
    try:
        label_type = ref_text[1]
        cor_list = eval(ref_text[2])
    except Exception as e:
        print(e)
        return None
    return (label_type, cor_list)


def draw_bounding_boxes(image, refs):
    """åœ¨å›¾åƒä¸Šç»˜åˆ¶è¾¹ç•Œæ¡†"""
    image_width, image_height = image.size
    img_draw = image.copy()
    draw = ImageDraw.Draw(img_draw)

    overlay = Image.new('RGBA', img_draw.size, (0, 0, 0, 0))
    draw2 = ImageDraw.Draw(overlay)

    font = ImageFont.load_default()

    cropped_images = []

    for i, ref in enumerate(refs):
        try:
            result = extract_coordinates_and_label(ref, image_width, image_height)
            if result:
                label_type, points_list = result

                # ä½¿ç”¨æ›´æ·±çš„é¢œè‰²èŒƒå›´ï¼Œæé«˜å¯è§åº¦
                color = (np.random.randint(100, 255), np.random.randint(100, 255), np.random.randint(100, 255))
                color_a = color + (60, )  # å¢åŠ åŠé€æ˜å¡«å……çš„ä¸é€æ˜åº¦

                for points in points_list:
                    x1, y1, x2, y2 = points

                    x1 = int(x1 / 999 * image_width)
                    y1 = int(y1 / 999 * image_height)
                    x2 = int(x2 / 999 * image_width)
                    y2 = int(y2 / 999 * image_height)

                    if label_type == 'image':
                        try:
                            cropped = image.crop((x1, y1, x2, y2))
                            cropped_images.append(cropped)
                        except Exception as e:
                            print(e)

                    try:
                        if label_type == 'title':
                            draw.rectangle([x1, y1, x2, y2], outline=color, width=4)
                            draw2.rectangle([x1, y1, x2, y2], fill=color_a, outline=(0, 0, 0, 0), width=1)
                        else:
                            draw.rectangle([x1, y1, x2, y2], outline=color, width=2)
                            draw2.rectangle([x1, y1, x2, y2], fill=color_a, outline=(0, 0, 0, 0), width=1)

                        text_x = x1
                        text_y = max(0, y1 - 15)

                        text_bbox = draw.textbbox((0, 0), label_type, font=font)
                        text_width = text_bbox[2] - text_bbox[0]
                        text_height = text_bbox[3] - text_bbox[1]
                        draw.rectangle([text_x, text_y, text_x + text_width, text_y + text_height],
                                     fill=(255, 255, 255, 30))

                        draw.text((text_x, text_y), label_type, font=font, fill=color)
                    except:
                        pass
        except:
            continue

    img_draw.paste(overlay, (0, 0), overlay)
    return img_draw, cropped_images


def start_event_loop():
    """åœ¨åå°çº¿ç¨‹å¯åŠ¨äº‹ä»¶å¾ªç¯"""
    global event_loop
    event_loop = asyncio.new_event_loop()
    asyncio.set_event_loop(event_loop)
    event_loop.run_forever()


def get_or_create_event_loop():
    """è·å–æˆ–åˆ›å»ºå…¨å±€äº‹ä»¶å¾ªç¯"""
    global event_loop, loop_thread

    if event_loop is None or not event_loop.is_running():
        with engine_lock:
            if event_loop is None or not event_loop.is_running():
                loop_thread = threading.Thread(target=start_event_loop, daemon=True)
                loop_thread.start()
                # ç­‰å¾…äº‹ä»¶å¾ªç¯å¯åŠ¨
                while event_loop is None:
                    time.sleep(0.1)

    return event_loop


async def initialize_engine(gpu_memory_utilization=0.75):
    """åˆå§‹åŒ–vLLMå¼•æ“"""
    global engine, processor

    if engine is None:
        print("æ­£åœ¨åˆå§‹åŒ–æ¨ç†å¼•æ“...")
        engine_args = AsyncEngineArgs(
            model=MODEL_PATH,
            hf_overrides={"architectures": ["DeepseekOCRForCausalLM"]},
            block_size=256,
            max_model_len=8192,
            enforce_eager=False,
            trust_remote_code=True,
            tensor_parallel_size=1,
            gpu_memory_utilization=gpu_memory_utilization,
        )
        engine = AsyncLLMEngine.from_engine_args(engine_args)
        print("æ¨ç†å¼•æ“åˆå§‹åŒ–å®Œæˆï¼")

    if processor is None:
        processor = DeepseekOCRProcessor()

    return engine, processor


async def stream_generate(image_features=None, prompt='', max_tokens=8192):
    """æµå¼ç”Ÿæˆæ–‡æœ¬"""
    global engine

    if engine is None:
        await initialize_engine()

    logits_processors = [NoRepeatNGramLogitsProcessor(ngram_size=30, window_size=90, whitelist_token_ids={128821, 128822})]

    sampling_params = SamplingParams(
        temperature=0.0,
        max_tokens=max_tokens,
        logits_processors=logits_processors,
        skip_special_tokens=False,
    )

    request_id = f"request-{int(time.time() * 1000)}"

    print(f"æäº¤è¯·æ±‚: {request_id}")

    if image_features and '<image>' in prompt:
        request = {
            "prompt": prompt,
            "multi_modal_data": {"image": image_features}
        }
    elif prompt:
        request = {
            "prompt": prompt
        }
    else:
        raise ValueError('æç¤ºè¯ä¸èƒ½ä¸ºç©ºï¼')

    full_text = ""
    printed_length = 0

    try:
        async for request_output in engine.generate(request, sampling_params, request_id):
            if request_output.outputs:
                full_text = request_output.outputs[0].text
                new_text = full_text[printed_length:]
                if new_text:
                    print(f"å·²ç”Ÿæˆ {len(new_text)} å­—ç¬¦...", end='\r')
                printed_length = len(full_text)

        print(f"\nç”Ÿæˆå®Œæˆ: å…± {len(full_text)} å­—ç¬¦")
    except Exception as e:
        print(f"ç”Ÿæˆè¿‡ç¨‹å‡ºé”™: {e}")
        raise

    return full_text


def run_async_task(coro):
    """åœ¨å…¨å±€äº‹ä»¶å¾ªç¯ä¸­è¿è¡Œå¼‚æ­¥ä»»åŠ¡"""
    loop = get_or_create_event_loop()
    future = asyncio.run_coroutine_threadsafe(coro, loop)
    return future.result()


def process_ocr(image, task_type, custom_prompt, base_size, image_size, crop_mode,
                min_crops, max_crops, gpu_memory):
    """å¤„ç†OCRä»»åŠ¡"""
    if image is None:
        return "è¯·å…ˆä¸Šä¼ å›¾ç‰‡ã€‚ / Please upload an image first.", None, None, None

    print(f"\n{'='*50}")
    print(f"æ­£åœ¨å¤„ç†æ–°è¯·æ±‚: {task_type}")
    print(f"{'='*50}")

    # åˆ›å»ºä¸´æ—¶ç›®å½•ä¿å­˜è£å‰ªçš„å›¾ç‰‡
    import tempfile
    import shutil
    temp_dir = tempfile.mkdtemp(prefix="deepseek_ocr_")
    print(f"ä¸´æ—¶ç›®å½•: {temp_dir}")

    # è®¾ç½®å…¨å±€é…ç½®
    import config
    config.BASE_SIZE = base_size
    config.IMAGE_SIZE = image_size
    config.CROP_MODE = crop_mode
    config.MIN_CROPS = min_crops
    config.MAX_CROPS = max_crops

    # åŠ è½½å›¾åƒ
    image_rgb = load_image(image)
    if image_rgb is None:
        shutil.rmtree(temp_dir, ignore_errors=True)
        return "åŠ è½½å›¾ç‰‡å¤±è´¥ã€‚ / Failed to load image.", None, None, None

    # Task name mapping (support both Chinese and English)
    task_map = {
        # Chinese
        "æ–‡æ¡£è½¬Markdown": "document_to_markdown",
        "OCRå¸¦å®šä½æ¡†": "ocr_with_grounding",
        "çº¯æ–‡æœ¬OCR": "plain_text_ocr",
        "å›¾è¡¨è§£æ": "figure_parsing",
        "å›¾åƒæè¿°": "image_description",
        "æ–‡æœ¬å®šä½": "text_localization",
        "ç‰©ä½“å®šä½": "object_localization",
        "è‡ªå®šä¹‰": "custom",
        # English
        "Document to Markdown": "document_to_markdown",
        "OCR with Grounding": "ocr_with_grounding",
        "Plain Text OCR": "plain_text_ocr",
        "Figure Parsing": "figure_parsing",
        "Image Description": "image_description",
        "Text Localization": "text_localization",
        "Object Localization": "object_localization",
        "Custom": "custom"
    }

    # Get normalized task type
    normalized_task = task_map.get(task_type, "ocr_with_grounding")

    # æ„å»ºprompt
    prompt_templates = {
        "document_to_markdown": "<image>\n<|grounding|>Convert the document to markdown.",
        "ocr_with_grounding": "<image>\n<|grounding|>OCR this image.",
        "plain_text_ocr": "<image>\nFree OCR.",
        "figure_parsing": "<image>\nParse the figure.",
        "image_description": "<image>\nDescribe this image in detail.",
        "text_localization": "<image>\nLocate text in the image.",
        "object_localization": "<image>\nLocate <|ref|>{target}<|/ref|> in the image.",
        "custom": custom_prompt if custom_prompt else "<image>\nOCR this image."
    }

    # ç‰¹æ®Šå¤„ç†ç‰©ä½“å®šä½ä»»åŠ¡
    if normalized_task == "object_localization":
        if custom_prompt and custom_prompt.strip():
            # ç”¨æˆ·è¾“å…¥çš„æ˜¯è¦å®šä½çš„ç›®æ ‡ç‰©ä½“
            prompt = f"<image>\nLocate <|ref|>{custom_prompt}<|/ref|> in the image."
        else:
            prompt = "<image>\nLocate objects in the image."
    else:
        prompt = prompt_templates.get(normalized_task, prompt_templates["ocr_with_grounding"])

    try:
        # åˆå§‹åŒ–å¼•æ“ï¼ˆä½¿ç”¨å…¨å±€äº‹ä»¶å¾ªç¯ï¼‰
        print("æ­£åœ¨åˆå§‹åŒ–å¼•æ“...")
        run_async_task(initialize_engine(gpu_memory_utilization=gpu_memory))

        # å¤„ç†å›¾åƒ
        if '<image>' in prompt:
            print("æ­£åœ¨å¤„ç†å›¾åƒç‰¹å¾...")
            config.PROMPT = prompt
            image_features = processor.tokenize_with_images(
                images=[image_rgb],
                bos=True,
                eos=True,
                cropping=crop_mode
            )
        else:
            image_features = None

        # ç”Ÿæˆç»“æœï¼ˆä½¿ç”¨å…¨å±€äº‹ä»¶å¾ªç¯ï¼‰
        print("æ­£åœ¨ç”Ÿæˆæ–‡æœ¬...")
        result_text = run_async_task(stream_generate(image_features, prompt))

        # å¤„ç†ç»“æœ
        result_with_boxes = None
        cropped_images = []
        clean_markdown = result_text

        if '<image>' in prompt and '<|ref|>' in result_text:
            # æå–åŒ¹é…é¡¹
            print("æ­£åœ¨å¤„ç†å®šä½æ ‡ç­¾...")
            matches_ref, matches_images, matches_other = re_match(result_text)

            # ç»˜åˆ¶è¾¹ç•Œæ¡†
            if matches_ref:
                result_with_boxes, cropped_images = draw_bounding_boxes(image_rgb, matches_ref)

            # æ¸…ç†markdown - ä¿ç•™æ–‡æœ¬å†…å®¹ï¼Œåªåˆ é™¤groundingæ ‡ç­¾
            clean_markdown = result_text

            # å¤„ç†å›¾åƒåŒ¹é…é¡¹ - å°†å›¾ç‰‡è½¬æ¢ä¸ºbase64åµŒå…¥åˆ°Markdownä¸­
            import base64
            from io import BytesIO

            for idx, match in enumerate(matches_images):
                if idx < len(cropped_images):
                    # ä¿å­˜è£å‰ªçš„å›¾ç‰‡åˆ°ä¸´æ—¶ç›®å½•ï¼ˆç”¨äºå¤‡ä»½ï¼‰
                    image_path = os.path.join(temp_dir, f'image_{idx}.jpg')
                    cropped_images[idx].save(image_path, 'JPEG', quality=95)
                    print(f"ä¿å­˜å›¾ç‰‡: {image_path}")

                    # å°†å›¾ç‰‡è½¬æ¢ä¸ºbase64ç¼–ç 
                    buffered = BytesIO()
                    cropped_images[idx].save(buffered, format="JPEG", quality=95)
                    img_base64 = base64.b64encode(buffered.getvalue()).decode()

                    # åœ¨Markdownä¸­ä½¿ç”¨data URIåµŒå…¥å›¾ç‰‡
                    clean_markdown = clean_markdown.replace(match, f'![å›¾ç‰‡ {idx}](data:image/jpeg;base64,{img_base64})\n\n')
                else:
                    clean_markdown = clean_markdown.replace(match, f'![å›¾ç‰‡ {idx}](image_{idx}.jpg)\n')

            # å¤„ç†å…¶ä»–åŒ¹é…é¡¹ - æå–refæ ‡ç­¾ä¸­çš„æ–‡æœ¬å†…å®¹
            for match in matches_other:
                # matchæ˜¯å®Œæ•´çš„æ ‡ç­¾: <|ref|>æ–‡æœ¬<|/ref|><|det|>åæ ‡<|/det|>
                # æˆ‘ä»¬éœ€è¦æå–"æ–‡æœ¬"éƒ¨åˆ†
                ref_pattern = r'<\|ref\|>(.*?)<\|/ref\|><\|det\|>.*?<\|/det\|>'
                ref_match = re.search(ref_pattern, match)
                if ref_match:
                    text_content = ref_match.group(1)
                    clean_markdown = clean_markdown.replace(match, text_content)
                else:
                    clean_markdown = clean_markdown.replace(match, '')

            # æ¸…ç†å…¶ä»–ç‰¹æ®Šç¬¦å·å’Œæ ‡è®°
            clean_markdown = clean_markdown.replace('\\coloneqq', ':=').replace('\\eqqcolon', '=:')

            # æ¸…ç†å¤šä½™çš„"text"æ ‡è®° - ç§»é™¤ç‹¬ç«‹è¡Œä¸Šçš„"text"
            clean_markdown = re.sub(r'(?m)^text\s*$', '', clean_markdown)
            clean_markdown = re.sub(r'(?m)^text\n', '', clean_markdown)

            # æ¸…ç†å¤šä½™çš„ç©ºè¡Œï¼ˆè¶…è¿‡2ä¸ªè¿ç»­ç©ºè¡Œçš„æƒ…å†µï¼‰
            clean_markdown = re.sub(r'\n{3,}', '\n\n', clean_markdown)

        print("å¤„ç†å®Œæˆï¼")
        print(f"è£å‰ªå›¾ç‰‡ä¿å­˜åœ¨: {temp_dir}")

        # ç¡®ä¿æ‰€æœ‰è¿”å›å€¼éƒ½æœ‰æ•ˆ
        final_markdown = clean_markdown if clean_markdown else ""
        final_boxes_image = result_with_boxes
        final_cropped = cropped_images if cropped_images else []
        final_raw = result_text if result_text else ""

        print(f"å‡†å¤‡è¿”å›ç»“æœ...")
        print(f"  - Markdown: {len(final_markdown)} å­—ç¬¦")
        print(f"  - æ ‡æ³¨å›¾åƒ: {'æœ‰' if final_boxes_image else 'æ— '}")
        print(f"  - è£å‰ªå›¾ç‰‡: {len(final_cropped)} ä¸ª")
        print(f"  - åŸå§‹è¾“å‡º: {len(final_raw)} å­—ç¬¦")

        return final_markdown, final_boxes_image, final_cropped, final_raw

    except Exception as e:
        print(f"é”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()
        shutil.rmtree(temp_dir, ignore_errors=True)
        return f"é”™è¯¯: {str(e)}", None, None, None


def create_gradio_interface():
    """åˆ›å»ºGradioç•Œé¢"""

    # åŒè¯­æ–‡æœ¬å­—å…¸
    texts = {
        'zh': {
            'title': 'DeepSeek-OCR: é«˜çº§è§†è§‰æ–‡æ¡£ç†è§£ç³»ç»Ÿ',
            'intro': '''æœ¬ç•Œé¢æ”¯æŒå¤šç§OCRå’Œæ–‡æ¡£ç†è§£ä»»åŠ¡ï¼š
- **æ–‡æ¡£è½¬Markdown**: å°†æ–‡æ¡£è½¬æ¢ä¸ºMarkdownæ ¼å¼ï¼Œä¿ç•™ç»“æ„å’Œå®šä½ä¿¡æ¯
- **OCRå¸¦å®šä½æ¡†**: æå–æ–‡æœ¬å¹¶æ ‡æ³¨è¾¹ç•Œæ¡†
- **çº¯æ–‡æœ¬OCR**: ç®€å•çš„æ–‡æœ¬æå–ï¼Œä¸ä¿ç•™å¸ƒå±€ä¿¡æ¯
- **å›¾è¡¨è§£æ**: åˆ†æå’Œæè¿°å›¾è¡¨ã€å›¾å½¢å’Œç¤ºæ„å›¾
- **å›¾åƒæè¿°**: ç”Ÿæˆè¯¦ç»†çš„å›¾åƒæè¿°
- **æ–‡æœ¬å®šä½**: å®šä½å’Œè¯†åˆ«æ–‡æœ¬åŒºåŸŸ
- **ç‰©ä½“å®šä½**: ğŸ†• å®šä½å›¾ç‰‡ä¸­çš„æŒ‡å®šç‰©ä½“ï¼ˆåœ¨è¾“å…¥æ¡†ä¸­å¡«å†™ç‰©ä½“åç§°ï¼Œå¦‚ï¼šæ‰‹æŒã€è‹¹æœã€äººè„¸ï¼‰
- **è‡ªå®šä¹‰**: ä½¿ç”¨æ‚¨è‡ªå·±çš„è‡ªå®šä¹‰æç¤ºè¯

**æ³¨æ„**: é¦–æ¬¡æ¨ç†éœ€è¦çº¦30-60ç§’è¿›è¡Œæ¨¡å‹ç¼–è¯‘ï¼Œåç»­è¿è¡Œä¼šå¿«å¾ˆå¤šï¼''',
            'input_section': 'è¾“å…¥',
            'output_section': 'ç»“æœ',
            'upload_image': 'ä¸Šä¼ å›¾ç‰‡',
            'task_type': 'ä»»åŠ¡ç±»å‹',
            'tasks': ['æ–‡æ¡£è½¬Markdown', 'OCRå¸¦å®šä½æ¡†', 'çº¯æ–‡æœ¬OCR', 'å›¾è¡¨è§£æ', 'å›¾åƒæè¿°', 'æ–‡æœ¬å®šä½', 'ç‰©ä½“å®šä½', 'è‡ªå®šä¹‰'],
            'custom_prompt_label': 'ç›®æ ‡ç‰©ä½“/è‡ªå®šä¹‰æç¤ºè¯',
            'custom_prompt_placeholder': 'ç‰©ä½“å®šä½ï¼šè¾“å…¥è¦å®šä½çš„ç‰©ä½“åç§°ï¼ˆå¦‚ï¼šæ‰‹æŒã€è‹¹æœã€æ±½è½¦ï¼‰\nè‡ªå®šä¹‰ï¼šè¾“å…¥å®Œæ•´æç¤ºè¯',
            'advanced_settings': 'é«˜çº§è®¾ç½®',
            'model_presets': '''**æ¨¡å‹å°ºå¯¸é¢„è®¾:**
- Tiny: base=512, image=512, ä¸è£å‰ª
- Small: base=640, image=640, ä¸è£å‰ª
- Base: base=1024, image=1024, ä¸è£å‰ª
- Large: base=1280, image=1280, ä¸è£å‰ª
- Gundam: base=1024, image=640, å¯ç”¨è£å‰ªï¼ˆæ¨èï¼‰''',
            'base_size': 'åŸºç¡€å°ºå¯¸',
            'image_size': 'å›¾åƒå°ºå¯¸',
            'crop_mode': 'å¯ç”¨åŠ¨æ€è£å‰ª',
            'min_crops': 'æœ€å°è£å‰ªæ•°',
            'max_crops': 'æœ€å¤§è£å‰ªæ•°',
            'gpu_memory': 'GPUæ˜¾å­˜åˆ©ç”¨ç‡',
            'process_btn': 'å¼€å§‹å¤„ç†',
            'markdown_output': 'Markdownè¾“å‡º',
            'clean_markdown': 'æ¸…ç†åçš„Markdown',
            'annotated_image': 'æ ‡æ³¨å›¾åƒ',
            'boxes_image': 'å¸¦è¾¹ç•Œæ¡†çš„å›¾åƒ',
            'extracted_images': 'æå–çš„å›¾åƒ',
            'cropped_regions': 'è£å‰ªåŒºåŸŸ',
            'raw_output': 'åŸå§‹è¾“å‡º',
            'raw_model_output': 'åŸå§‹æ¨¡å‹è¾“å‡º',
        },
        'en': {
            'title': 'DeepSeek-OCR: Advanced Visual Document Understanding',
            'intro': '''This interface supports multiple OCR and document understanding tasks:
- **Document to Markdown**: Convert documents to Markdown format with structure and location info
- **OCR with Grounding**: Extract text with bounding box annotations
- **Plain Text OCR**: Simple text extraction without layout preservation
- **Figure Parsing**: Analyze and describe charts, diagrams, and schematics
- **Image Description**: Generate detailed image descriptions
- **Text Localization**: Locate and recognize text regions
- **Object Localization**: ğŸ†• Locate specified objects in images (enter object name in input box, e.g.: palm, apple, face)
- **Custom**: Use your own custom prompts

**Note**: First inference takes ~30-60s for model compilation, subsequent runs are much faster!''',
            'input_section': 'Input',
            'output_section': 'Output',
            'upload_image': 'Upload Image',
            'task_type': 'Task Type',
            'tasks': ['Document to Markdown', 'OCR with Grounding', 'Plain Text OCR', 'Figure Parsing', 'Image Description', 'Text Localization', 'Object Localization', 'Custom'],
            'custom_prompt_label': 'Target Object/Custom Prompt',
            'custom_prompt_placeholder': 'Object Localization: Enter object name (e.g.: palm, apple, car)\nCustom: Enter full prompt',
            'advanced_settings': 'Advanced Settings',
            'model_presets': '''**Model Size Presets:**
- Tiny: base=512, image=512, no cropping
- Small: base=640, image=640, no cropping
- Base: base=1024, image=1024, no cropping
- Large: base=1280, image=1280, no cropping
- Gundam: base=1024, image=640, cropping enabled (recommended)''',
            'base_size': 'Base Size',
            'image_size': 'Image Size',
            'crop_mode': 'Enable Dynamic Cropping',
            'min_crops': 'Minimum Crops',
            'max_crops': 'Maximum Crops',
            'gpu_memory': 'GPU Memory Utilization',
            'process_btn': 'Process',
            'markdown_output': 'Markdown Output',
            'clean_markdown': 'Cleaned Markdown',
            'annotated_image': 'Annotated Image',
            'boxes_image': 'Image with Bounding Boxes',
            'extracted_images': 'Extracted Images',
            'cropped_regions': 'Cropped Regions',
            'raw_output': 'Raw Output',
            'raw_model_output': 'Raw Model Output',
        }
    }

    # CSS for centered title
    custom_css = """
    <style>
    .title-center {
        text-align: center;
        margin-bottom: 1em;
    }
    .lang-selector {
        position: absolute;
        top: 20px;
        right: 20px;
    }
    </style>
    """

    with gr.Blocks(title="DeepSeek-OCR", theme=gr.themes.Soft(), css=custom_css) as demo:
        # Language state
        current_lang = gr.State('zh')

        # Language selector
        with gr.Row():
            with gr.Column(scale=4):
                title_md = gr.Markdown(f'<h1 class="title-center">{texts["zh"]["title"]}</h1>')
            with gr.Column(scale=1, elem_classes="lang-selector"):
                language = gr.Radio(
                    choices=["ä¸­æ–‡", "English"],
                    value="ä¸­æ–‡",
                    label="Language / è¯­è¨€",
                    container=False
                )

        intro_md = gr.Markdown(texts['zh']['intro'])

        with gr.Row():
            with gr.Column(scale=1):
                # è¾“å…¥åŒºåŸŸ
                input_section_md = gr.Markdown(f"### {texts['zh']['input_section']}")
                image_input = gr.Image(type="pil", label=texts['zh']['upload_image'])

                task_type = gr.Dropdown(
                    choices=texts['zh']['tasks'],
                    value=texts['zh']['tasks'][0],
                    label=texts['zh']['task_type']
                )

                custom_prompt = gr.Textbox(
                    label=texts['zh']['custom_prompt_label'],
                    placeholder=texts['zh']['custom_prompt_placeholder'],
                    lines=2
                )

                # é«˜çº§è®¾ç½®
                with gr.Accordion(texts['zh']['advanced_settings'], open=False) as advanced_accordion:
                    model_presets_md = gr.Markdown(texts['zh']['model_presets'])

                    base_size = gr.Slider(
                        minimum=512,
                        maximum=1536,
                        value=1024,
                        step=128,
                        label=texts['zh']['base_size']
                    )

                    image_size = gr.Slider(
                        minimum=512,
                        maximum=1536,
                        value=640,
                        step=128,
                        label=texts['zh']['image_size']
                    )

                    crop_mode = gr.Checkbox(
                        value=True,
                        label=texts['zh']['crop_mode']
                    )

                    min_crops = gr.Slider(
                        minimum=1,
                        maximum=9,
                        value=2,
                        step=1,
                        label=texts['zh']['min_crops']
                    )

                    max_crops = gr.Slider(
                        minimum=1,
                        maximum=9,
                        value=6,
                        step=1,
                        label=texts['zh']['max_crops']
                    )

                    gpu_memory = gr.Slider(
                        minimum=0.3,
                        maximum=0.95,
                        value=0.75,
                        step=0.05,
                        label=texts['zh']['gpu_memory']
                    )

                process_btn = gr.Button(texts['zh']['process_btn'], variant="primary", size="lg")

            with gr.Column(scale=2):
                # è¾“å‡ºåŒºåŸŸ
                output_section_md = gr.Markdown(f"### {texts['zh']['output_section']}")

                with gr.Tabs():
                    with gr.Tab(texts['zh']['markdown_output']) as tab_markdown:
                        markdown_output = gr.Markdown(
                            label=texts['zh']['clean_markdown'],
                            value=""
                        )

                    with gr.Tab(texts['zh']['annotated_image']) as tab_annotated:
                        image_with_boxes = gr.Image(
                            label=texts['zh']['boxes_image'],
                            type="pil",
                            height=600
                        )

                    with gr.Tab(texts['zh']['extracted_images']) as tab_extracted:
                        cropped_gallery = gr.Gallery(
                            label=texts['zh']['cropped_regions'],
                            columns=3,
                            height="auto"
                        )

                    with gr.Tab(texts['zh']['raw_output']) as tab_raw:
                        raw_output = gr.Textbox(
                            label=texts['zh']['raw_model_output'],
                            lines=20,
                            max_lines=30
                        )


        # è¯­è¨€åˆ‡æ¢åŠŸèƒ½
        def update_language(lang_choice):
            lang = 'zh' if lang_choice == 'ä¸­æ–‡' else 'en'
            t = texts[lang]

            return [
                f'<h1 class="title-center">{t["title"]}</h1>',  # title_md
                t['intro'],  # intro_md
                f"### {t['input_section']}",  # input_section_md
                gr.update(label=t['upload_image']),  # image_input
                gr.update(choices=t['tasks'], value=t['tasks'][0], label=t['task_type']),  # task_type
                gr.update(label=t['custom_prompt_label'], placeholder=t['custom_prompt_placeholder']),  # custom_prompt
                gr.update(label=t['advanced_settings']),  # advanced_accordion
                t['model_presets'],  # model_presets_md
                gr.update(label=t['base_size']),  # base_size
                gr.update(label=t['image_size']),  # image_size
                gr.update(label=t['crop_mode']),  # crop_mode
                gr.update(label=t['min_crops']),  # min_crops
                gr.update(label=t['max_crops']),  # max_crops
                gr.update(label=t['gpu_memory']),  # gpu_memory
                gr.update(value=t['process_btn']),  # process_btn
                f"### {t['output_section']}",  # output_section_md
                gr.update(label=t['markdown_output']),  # tab_markdown
                gr.update(label=t['clean_markdown']),  # markdown_output
                gr.update(label=t['annotated_image']),  # tab_annotated
                gr.update(label=t['boxes_image']),  # image_with_boxes
                gr.update(label=t['extracted_images']),  # tab_extracted
                gr.update(label=t['cropped_regions']),  # cropped_gallery
                gr.update(label=t['raw_output']),  # tab_raw
                gr.update(label=t['raw_model_output']),  # raw_output
            ]

        language.change(
            fn=update_language,
            inputs=[language],
            outputs=[
                title_md, intro_md, input_section_md, image_input, task_type, custom_prompt,
                advanced_accordion, model_presets_md, base_size, image_size, crop_mode,
                min_crops, max_crops, gpu_memory, process_btn, output_section_md,
                tab_markdown, markdown_output, tab_annotated, image_with_boxes,
                tab_extracted, cropped_gallery, tab_raw, raw_output
            ]
        )

        # ç»‘å®šå¤„ç†å‡½æ•°
        process_btn.click(
            fn=process_ocr,
            inputs=[
                image_input, task_type, custom_prompt, base_size, image_size,
                crop_mode, min_crops, max_crops, gpu_memory
            ],
            outputs=[markdown_output, image_with_boxes, cropped_gallery, raw_output]
        )

    return demo


if __name__ == "__main__":
    # é¢„å…ˆåˆ›å»ºäº‹ä»¶å¾ªç¯
    get_or_create_event_loop()

    demo = create_gradio_interface()
    demo.queue()
    demo.launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=False
    )
